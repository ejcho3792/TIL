Index: ProDS/Set01_Set05_수정(1).py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ProDS/Set01_Set05_수정(1).py b/ProDS/Set01_Set05_수정(1).py
new file mode 100644
--- /dev/null	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
+++ b/ProDS/Set01_Set05_수정(1).py	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
@@ -0,0 +1,629 @@
+# -*- coding: utf-8 -*-
+"""
+Created on 2021
+
+@author: Administrator
+"""
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 01 유형(DataSet_01.csv 이용)
+#
+# 구분자 : comma(“,”), 4,572 Rows, 5 Columns, UTF-8 인코딩
+# 
+# 글로벌 전자제품 제조회사에서 효과적인 마케팅 방법을 찾기
+# 위해서 채널별 마케팅 예산과 매출금액과의 관계를 분석하고자
+# 한다.
+# 컬 럼 / 정 의  /   Type
+# TV   /     TV 마케팅 예산 (억원)  /   Double
+# Radio / 라디오 마케팅 예산 (억원)  /   Double
+# Social_Media / 소셜미디어 마케팅 예산 (억원)  / Double
+# Influencer / 인플루언서 마케팅
+# (인플루언서의 영향력 크기에 따라 Mega / Macro / Micro / 
+# Nano) / String
+
+# SALES / 매출액 / Double
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data1=pd.read_csv('Dataset_01.csv')
+data1.info()
+data1.columns
+# ['TV', 'Radio', 'Social_Media', 'Influencer', 'Sales']
+#%%
+
+# =============================================================================
+# 1. 데이터 세트 내에 총 결측값의 개수는 몇 개인가? (답안 예시) 23
+# =============================================================================
+
+data1.isna().sum().sum()
+
+# 답: 26
+
+# [참고] 결측치가 포함된 행의 수
+data1.isna().any(axis=1).sum()
+
+
+#%%
+
+# =============================================================================
+# 2. TV, Radio, Social Media 등 세 가지 다른 마케팅 채널의 예산과 매출액과의 상관분석을
+# 통하여 각 채널이 매출에 어느 정도 연관이 있는지 알아보고자 한다. 
+# - 매출액과 가장 강한 상관관계를 가지고 있는 채널의 상관계수를 소수점 5번째
+# 자리에서 반올림하여 소수점 넷째 자리까지 기술하시오. (답안 예시) 0.1234
+# =============================================================================
+
+
+var_list=['TV', 'Radio', 'Social_Media','Sales']
+
+q2=data1[var_list].corr().abs().drop('Sales')['Sales']
+
+round(q2.max(), 4) # 최대값
+
+# 답: 0.9995
+
+# [참고]
+q2.nlargest(1) # 상위 n 개
+q2.argmax() # 최대값이 있는 위치번호
+q2.idxmax() # 최대값이 있는 인덱스명
+
+
+
+#%%
+
+# =============================================================================
+# 3. 매출액을 종속변수, TV, Radio, Social Media의 예산을 독립변수로 하여 회귀분석을
+# 수행하였을 때, 세 개의 독립변수의 회귀계수를 큰 것에서부터 작은 것 순으로
+# 기술하시오. 
+# - 분석 시 결측치가 포함된 행은 제거한 후 진행하며, 회귀계수는 소수점 넷째 자리
+# 이하는 버리고 소수점 셋째 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+from sklearn.linear_model import LinearRegression
+from statsmodels.formula.api import ols
+from statsmodels.api import OLS, add_constant
+
+# LinearRegression
+
+q3=data1.dropna()
+var_list=['TV', 'Radio', 'Social_Media']
+
+
+lm=LinearRegression(fit_intercept=True)
+lm.fit(q3[var_list], q3['Sales'])
+
+dir(lm)
+
+lm.intercept_ # 절편
+lm.coef_ # 회귀계수
+
+# [ 3.56256963, -0.00397039,  0.00496402]
+
+# ols
+
+# lm2=ols(form, data)
+# lm3=lm2.fit()
+# lm2=ols(form, data).fit()
+# form :  'y~x1+C(x2)+x3-1'
+
+form1='Sales~'+'+'.join(var_list)
+print(form1) # Sales~TV+Radio+Social_Media
+
+lm2=ols(form1, q3).fit()
+
+dir(lm2)
+lm2.summary()
+
+# OLS: 절편 미포함 
+# - 절편 구할 수 있도록 1만 들어가 있는 상수항 변수 추가
+
+X=add_constant(q3[var_list])
+lm3=OLS(q3['Sales'], X).fit()
+lm3.summary()
+
+
+lm2.params
+lm2.pvalues[lm2.pvalues < 0.05]
+
+lm2.params.drop('Intercept').sort_values(ascending=False)
+
+# 답: 
+# TV              3.562570
+# Social_Media    0.004964
+# Radio          -0.003970
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 02 유형(DataSet_02.csv 이용)
+# 구분자 : comma(“,”), 200 Rows, 6 Columns, UTF-8 인코딩
+
+# 환자의 상태와 그에 따라 처방된 약에 대한 정보를 분석하고자한다
+# 
+# 컬 럼 / 정 의  / Type
+# Age  / 연령 / Integer
+# Sex / 성별 / String
+# BP / 혈압 레벨 / String
+# Cholesterol / 콜레스테롤 레벨 /  String
+# Na_to_k / 혈액 내 칼륨에 대비한 나트륨 비율 / Double
+# Drug / Drug Type / String
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data2=pd.read_csv('Dataset_02.csv')
+
+
+#%%
+
+# =============================================================================
+# 1.해당 데이터에 대한 EDA를 수행하고, 여성으로 혈압이 High, Cholesterol이 Normal인
+# 환자의 전체에 대비한 비율이 얼마인지 소수점 네 번째 자리에서 반올림하여 소수점 셋째
+# 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+data2.columns
+# ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug']
+
+var_list=['Sex', 'BP', 'Cholesterol']
+q1=data2[var_list].value_counts(normalize=True)
+q1
+
+q1[('F', 'HIGH', 'NORMAL')]
+
+# 답: 0.105
+
+
+#%%
+
+# =============================================================================
+# 2. Age, Sex, BP, Cholesterol 및 Na_to_k 값이 Drug 타입에 영향을 미치는지 확인하기
+# 위하여 아래와 같이 데이터를 변환하고 분석을 수행하시오. 
+# - Age_gr 컬럼을 만들고, Age가 20 미만은 ‘10’, 20부터 30 미만은 ‘20’, 30부터 40 미만은
+# ‘30’, 40부터 50 미만은 ‘40’, 50부터 60 미만은 ‘50’, 60이상은 ‘60’으로 변환하시오. 
+# - Na_K_gr 컬럼을 만들고 Na_to_k 값이 10이하는 ‘Lv1’, 20이하는 ‘Lv2’, 30이하는 ‘Lv3’, 30 
+# 초과는 ‘Lv4’로 변환하시오.
+# - Sex, BP, Cholesterol, Age_gr, Na_K_gr이 Drug 변수와 영향이 있는지 독립성 검정을
+# 수행하시오.
+# - 검정 수행 결과, Drug 타입과 연관성이 있는 변수는 몇 개인가? 연관성이 있는 변수
+# 가운데 가장 큰 p-value를 찾아 소수점 여섯 번째 자리 이하는 버리고 소수점 다섯
+# 번째 자리까지 기술하시오.
+# (답안 예시) 3, 1.23456
+# =============================================================================
+
+q2=data2.copy()
+
+# 변수 생성
+
+q2['Age_gr']=np.where(q2.Age < 20, 10, 
+                np.where(q2.Age < 30, 20,
+                   np.where(q2.Age < 40, 30,
+                      np.where(q2.Age < 50, 40, 
+                         np.where(q2.Age < 60, 50, 60)))))
+
+q2['Na_K_gr']=np.where(q2.Na_to_K <= 10, 'Lv1',
+                 np.where(q2.Na_to_K <= 20, 'Lv2',
+                    np.where(q2.Na_to_K <= 30, 'Lv3', 'Lv4')))      
+
+# 교차표 작성
+q2.columns
+# ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug', 'Age_gr',
+#       'Na_K_gr']
+
+
+temp=pd.crosstab(index=q2['Sex'],
+                 columns=q2['Drug'])
+
+# 카이스퀘어 검정
+from scipy.stats import chi2_contingency
+
+q2_out=chi2_contingency(temp)
+q2_out[1]  # pvalue
+
+# 반복 적용
+from scipy.stats import chi2_contingency
+
+var_list=['Sex', 'BP', 'Cholesterol', 'Age_gr', 'Na_K_gr']
+
+q2_out2=[]
+for i in var_list:
+    temp=pd.crosstab(index=q2[i],
+                     columns=q2['Drug'])
+    q2_out=chi2_contingency(temp)
+    chi2=q2_out[0]
+    pvalue=q2_out[1]
+    q2_out2.append([i, chi2, pvalue])
+
+q2_out2=pd.DataFrame(q2_out2, columns=['var', 'chi2', 'pvalue'])
+
+# 영향력 있는 변수 추출
+q2_out3=q2_out2[q2_out2.pvalue < 0.05]
+len(q2_out3)  # 4
+
+# 영향력 변수 중에 가장 큰 p-value
+q2_out3.pvalue.max() # 0.0007010113024729462
+
+# 답: 4, 0.00070
+
+#%%
+
+# =============================================================================
+# 3.Sex, BP, Cholesterol 등 세 개의 변수를 다음과 같이 변환하고 의사결정나무를 이용한
+# 분석을 수행하시오.
+# - Sex는 M을 0, F를 1로 변환하여 Sex_cd 변수 생성
+# - BP는 LOW는 0, NORMAL은 1 그리고 HIGH는 2로 변환하여 BP_cd 변수 생성
+# - Cholesterol은 NORMAL은 0, HIGH는 1로 변환하여 Ch_cd 생성
+# - Age, Na_to_k, Sex_cd, BP_cd, Ch_cd를 Feature로, Drug을 Label로 하여 의사결정나무를
+# 수행하고 Root Node의 split feature와 split value를 기술하시오. 
+# 이 때 split value는 소수점 셋째 자리까지 반올림하여 기술하시오. (답안 예시) Age, 
+# 12.345
+# =============================================================================
+
+q3=data2.copy()
+
+# 변수 생성
+q3['Sex_cd']=np.where(q3.Sex == 'M', 0, 1)
+q3['BP_cd']=np.where(q3.BP == 'LOW', 0,
+                np.where(q3.BP == 'NORMAL', 1, 2))
+q3['Ch_cd']=np.where(q3.Cholesterol == 'NORMAL', 0, 1)
+
+
+# 의사결정나무 실행 -> 모델 생성
+from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
+
+var_list=['Age', 'Na_to_K', 'Sex_cd', 'BP_cd', 'Ch_cd']
+
+dt=DecisionTreeClassifier().fit(q3[var_list], q3['Drug'])
+
+
+# Root Node의 split feature와 split value
+
+
+plot_tree(dt, max_depth=1, feature_names=var_list,
+          class_names=list(q3.Drug.unique()),
+          precision=3,
+          fontsize=8)
+
+
+print(export_text(dt, feature_names=var_list, decimals=3))
+
+# 답: Na_to_K , 14.829
+
+dt.feature_importances_
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 03 유형(DataSet_03.csv 이용)
+# 
+# 구분자 : comma(“,”), 5,001 Rows, 8 Columns, UTF-8 인코딩
+# 안경 체인을 운영하고 있는 한 회사에서 고객 사진을 바탕으로 안경의 사이즈를
+# 맞춤 제작하는 비즈니스를 기획하고 있다. 우선 데이터만으로 고객의 성별을
+# 파악하는 것이 가능할 지를 연구하고자 한다.
+#
+# 컬 럼 / 정 의 / Type
+# long_hair / 머리카락 길이 (0 – 길지 않은 경우 / 1 – 긴
+# 경우) / Integer
+# forehead_width_cm / 이마의 폭 (cm) / Double
+# forehead_height_cm / 이마의 높이 (cm) / Double
+# nose_wide / 코의 넓이 (0 – 넓지 않은 경우 / 1 – 넓은 경우) / Integer
+# nose_long / 코의 길이 (0 – 길지 않은 경우 / 1 – 긴 경우) / Integer
+# lips_thin / 입술이 얇은지 여부 0 – 얇지 않은 경우 / 1 –
+# 얇은 경우) / Integer
+# distance_nose_to_lip_long / 인중의 길이(0 – 인중이 짧은 경우 / 1 – 인중이
+# 긴 경우) / Integer
+# gender / 성별 (Female / Male) / String
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data3=pd.read_csv('Dataset_03.csv')
+
+#%%
+
+# =============================================================================
+# 1.이마의 폭(forehead_width_cm)과 높이(forehead_height_cm) 사이의
+# 비율(forehead_ratio)에 대해서 평균으로부터 3 표준편차 밖의 경우를 이상치로
+# 정의할 때, 이상치에 해당하는 데이터는 몇 개인가? (답안 예시) 10
+# =============================================================================
+
+q1=data3.copy()
+
+q1['forehead_ratio']=q1['forehead_width_cm']/q1['forehead_height_cm']
+
+xbar=q1['forehead_ratio'].mean()
+std=q1['forehead_ratio'].std()
+
+UB=xbar + 3 * std
+LB=xbar - 3 * std
+
+# 이상치 데이터 필터링
+q1[(q1['forehead_ratio'] < LB)  | (q1['forehead_ratio'] > UB)]
+
+# 이상치 수
+((q1['forehead_ratio'] < LB)  | (q1['forehead_ratio'] > UB)).sum()
+
+
+# 답 : 3
+#%%
+
+# =============================================================================
+# 2.성별에 따라 forehead_ratio 평균에 차이가 있는지 적절한 통계 검정을 수행하시오.
+# - 검정은 이분산을 가정하고 수행한다.
+# - 검정통계량의 추정치는 절대값을 취한 후 소수점 셋째 자리까지 반올림하여
+# 기술하시오.
+# - 신뢰수준 99%에서 양측 검정을 수행하고 결과는 귀무가설 기각의 경우 Y로, 그렇지
+# 않을 경우 N으로 답하시오. (답안 예시) 1.234, Y
+# =============================================================================
+q1.columns
+# ['long_hair', 'forehead_width_cm', 'forehead_height_cm', 'nose_wide',
+#       'nose_long', 'lips_thin', 'distance_nose_to_lip_long', 'gender',
+#       'forehead_ratio']
+q1.gender.unique()
+
+g_m=q1[q1.gender=='Male']['forehead_ratio']
+g_f=q1[q1.gender=='Female']['forehead_ratio']
+
+
+from scipy.stats import ttest_ind, bartlett
+
+q2_out=ttest_ind(g_m, g_f, equal_var=False)
+
+round(abs(q2_out.statistic),3 )
+
+# 답: 2.999
+
+
+# [참고] 등분산 검정
+bartlett(g_m, g_f)
+# H0 : 등분산
+#H1 : 등분산이 아니다(이분산)
+
+
+#%%
+
+# =============================================================================
+# 3.주어진 데이터를 사용하여 성별을 구분할 수 있는지 로지스틱 회귀분석을 적용하여
+# 알아 보고자 한다. 
+# - 데이터를 7대 3으로 나누어 각각 Train과 Test set로 사용한다. 이 때 seed는 123으로
+# 한다.
+# - 원 데이터에 있는 7개의 변수만 Feature로 사용하고 gender를 label로 사용한다.
+# (forehead_ratio는 사용하지 않음)
+# - 로지스틱 회귀분석 예측 함수와 Test dataset를 사용하여 예측을 수행하고 정확도를
+# 평가한다. 이 때 임계값은 0.5를 사용한다. 
+# - Male의 Precision 값을 소수점 둘째 자리까지 반올림하여 기술하시오. (답안 예시) 
+# 0.12
+# 
+# 
+# (참고) 
+# from sklearn.linear_model import LogisticRegression
+# from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+# train_test_split 의 random_state = 123
+# =============================================================================
+
+
+from sklearn.model_selection import train_test_split
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import precision_score, classification_report
+
+
+train, test=train_test_split(data3, test_size=0.3, random_state=123)
+
+var_list=data3.columns.drop('gender')
+
+logit=LogisticRegression().fit(train[var_list], train['gender'])
+
+dir(logit)
+
+q3_pred=logit.predict(test[var_list])
+
+q3_pred_pr=logit.predict_proba(test[var_list])
+
+precision_score(test['gender'], q3_pred, pos_label='Male')
+
+# 답: 0.96
+
+print(classification_report(test['gender'], q3_pred))
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 04 유형(DataSet_04.csv 이용)
+#
+#구분자 : comma(“,”), 6,718 Rows, 4 Columns, UTF-8 인코딩
+
+# 한국인의 식생활 변화가 건강에 미치는 영향을 분석하기에 앞서 육류
+# 소비량에 대한 분석을 하려고 한다. 확보한 데이터는 세계 각국의 1인당
+# 육류 소비량 데이터로 아래와 같은 내용을 담고 있다.
+
+# 컬 럼 / 정 의 / Type
+# LOCATION / 국가명 / String
+# SUBJECT / 육류 종류 (BEEF / PIG / POULTRY / SHEEP) / String
+# TIME / 연도 (1990 ~ 2026) / Integer
+# Value / 1인당 육류 소비량 (KG) / Double
+# =============================================================================
+# =============================================================================
+
+# (참고)
+# #1
+# import pandas as pd
+# import numpy as np
+# #2
+# from scipy.stats import ttest_rel
+# #3
+# from sklearn.linear_model import LinearRegression
+
+#%%
+
+import pandas as pd
+import numpy as np
+
+data4=pd.read_csv('Dataset_04.csv')
+
+
+# =============================================================================
+# 1.한국인의 1인당 육류 소비량이 해가 갈수록 증가하는 것으로 보여 상관분석을 통하여
+# 확인하려고 한다. 
+# - 데이터 파일로부터 한국 데이터만 추출한다. 한국은 KOR로 표기되어 있다.
+# - 년도별 육류 소비량 합계를 구하여 TIME과 Value간의 상관분석을 수행하고
+# 상관계수를 소수점 셋째 자리에서 반올림하여 소수점 둘째 자리까지만 기술하시오. 
+# (답안 예시) 0.55
+# =============================================================================
+
+
+data4.columns
+# ['LOCATION', 'SUBJECT', 'TIME', 'Value']
+
+q1=data4[data4.LOCATION=='KOR']
+
+q1_out=q1.groupby('TIME')['Value'].sum().reset_index().corr()
+
+q1_out['TIME']['Value']
+
+# 답: 0.96
+
+#%%
+
+# =============================================================================
+# 2. 한국 인근 국가 가운데 식생의 유사성이 상대적으로 높은 일본(JPN)과 비교하여, 연도별
+# 소비량에 평균 차이가 있는지 분석하고자 한다.
+# - 두 국가의 육류별 소비량을 연도기준으로 비교하는 대응표본 t 검정을 수행하시오.
+# - 두 국가 간의 연도별 소비량 차이가 없는 것으로 판단할 수 있는 육류 종류를 모두
+# 적으시오. (알파벳 순서) (답안 예시) BEEF, PIG, POULTRY, SHEEP
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.(한국만 포함한 데이터에서) Time을 독립변수로, Value를 종속변수로 하여 육류
+# 종류(SUBJECT) 별로 회귀분석을 수행하였을 때, 가장 높은 결정계수를 가진 모델의
+# 학습오차 중 MAPE를 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 21.12
+# (MAPE : Mean Absolute Percentage Error, 평균 절대 백분율 오차)
+# (MAPE = Σ ( | y - y ̂ | / y ) * 100/n ))
+# 
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 05 유형(DataSet_05.csv 이용)
+#
+# 구분자 : comma(“,”), 8,068 Rows, 12 Columns, UTF-8 인코딩
+#
+# A자동차 회사는 신규 진입하는 시장에 기존 모델을 판매하기 위한 마케팅 전략을 
+# 세우려고 한다. 기존 시장과 고객 특성이 유사하다는 전제 하에 기존 고객을 세분화하여
+# 각 그룹의 특징을 파악하고, 이를 이용하여 신규 진입 시장의 마케팅 계획을 
+# 수립하고자 한다. 다음은 기존 시장 고객에 대한 데이터이다.
+#
+
+# 컬 럼 / 정 의 / Type
+# ID / 고유 식별자 / Double
+# Age / 나이 / Double
+# Age_gr / 나이 그룹 (10/20/30/40/50/60/70) / Double
+# Gender / 성별 (여성 : 0 / 남성 : 1) / Double
+# Work_Experience / 취업 연수 (0 ~ 14) / Double
+# Family_Size / 가족 규모 (1 ~ 9) / Double
+# Ever_Married / 결혼 여부 (Unknown : 0 / No : 1 / Yes : 2) / Double
+# Graduated / 재학 중인지 여부 / Double
+# Profession / 직업 (Unknown : 0 / Artist ~ Marketing 등 9개) / Double
+# Spending_Score / 소비 점수 (Average : 0 / High : 1 / Low : 2) / Double
+# Var_1 / 내용이 알려지지 않은 고객 분류 코드 (0 ~ 7) / Double
+# Segmentation / 고객 세분화 결과 (A ~ D) / String
+# =============================================================================
+# =============================================================================
+
+
+#(참고)
+#1
+# import pandas as pd
+# #2
+# from scipy.stats import chi2_contingency
+# #3
+# from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+# from sklearn.tree import DecisionTreeClassifier
+# from sklearn.tree import export_graphviz
+# import pydot
+
+
+#%%
+
+# =============================================================================
+# 1.위의 표에 표시된 데이터 타입에 맞도록 전처리를 수행하였을 때, 데이터 파일 내에
+# 존재하는 결측값은 모두 몇 개인가? 숫자형 데이터와 문자열 데이터의 결측값을
+# 모두 더하여 답하시오.
+# (String 타입 변수의 경우 White Space(Blank)를 결측으로 처리한다) (답안 예시) 123
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.이어지는 분석을 위해 결측값을 모두 삭제한다. 그리고, 성별이 세분화(Segmentation)에
+# 영향을 미치는지 독립성 검정을 수행한다. 수행 결과, p-value를 반올림하여 소수점
+# 넷째 자리까지 쓰고, 귀무가설을 기각하면 Y로, 기각할 수 없으면 N으로 기술하시오. 
+# (답안 예시) 0.2345, N
+# =============================================================================
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.Segmentation 값이 A 또는 D인 데이터만 사용하여 의사결정 나무 기법으로 분류
+# 정확도를
+# 측정해 본다. 
+# - 결측치가 포함된 행은 제거한 후 진행하시오.
+# - Train대 Test 7대3으로 데이터를 분리한다. (Seed = 123)
+# - Train 데이터를 사용하여 의사결정나무 학습을 수행하고, Test 데이터로 평가를
+# 수행한다.
+# - 의사결정나무 학습 시, 다음과 같이 설정하시오:
+# • Feature: Age_gr, Gender, Work_Experience, Family_Size, 
+#             Ever_Married, Graduated, Spending_Score
+# • Label : Segmentation
+# • Parameter : Gini / Max Depth = 7 / Seed = 123
+# 이 때 전체 정확도(Accuracy)를 소수점 셋째 자리 이하는 버리고 소수점 둘째자리까지
+# 기술하시오.
+# (답안 예시) 0.12
+# =============================================================================
+
+
+
Index: ProDS/Set01_Set05_수정.py
===================================================================
diff --git a/ProDS/Set01_Set05_수정.py b/ProDS/Set01_Set05_수정.py
new file mode 100644
--- /dev/null	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
+++ b/ProDS/Set01_Set05_수정.py	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
@@ -0,0 +1,580 @@
+# -*- coding: utf-8 -*-
+"""
+Created on 2021
+
+@author: Administrator
+"""
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 01 유형(DataSet_01.csv 이용)
+#
+# 구분자 : comma(“,”), 4,572 Rows, 5 Columns, UTF-8 인코딩
+# 
+# 글로벌 전자제품 제조회사에서 효과적인 마케팅 방법을 찾기
+# 위해서 채널별 마케팅 예산과 매출금액과의 관계를 분석하고자
+# 한다.
+# 컬 럼 / 정 의  /   Type
+# TV   /     TV 마케팅 예산 (억원)  /   Double
+# Radio / 라디오 마케팅 예산 (억원)  /   Double
+# Social_Media / 소셜미디어 마케팅 예산 (억원)  / Double
+# Influencer / 인플루언서 마케팅
+# (인플루언서의 영향력 크기에 따라 Mega / Macro / Micro / 
+# Nano) / String
+
+# SALES / 매출액 / Double
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data1=pd.read_csv('Dataset_01.csv')
+data1.info()
+data1.columns
+# ['TV', 'Radio', 'Social_Media', 'Influencer', 'Sales']
+#%%
+
+# =============================================================================
+# 1. 데이터 세트 내에 총 결측값의 개수는 몇 개인가? (답안 예시) 23
+# =============================================================================
+
+data1.isna().sum().sum()
+
+# 답: 26
+
+# [참고] 결측치가 포함된 행의 수
+data1.isna().any(axis=1).sum()
+
+
+#%%
+
+# =============================================================================
+# 2. TV, Radio, Social Media 등 세 가지 다른 마케팅 채널의 예산과 매출액과의 상관분석을
+# 통하여 각 채널이 매출에 어느 정도 연관이 있는지 알아보고자 한다. 
+# - 매출액과 가장 강한 상관관계를 가지고 있는 채널의 상관계수를 소수점 5번째
+# 자리에서 반올림하여 소수점 넷째 자리까지 기술하시오. (답안 예시) 0.1234
+# =============================================================================
+
+
+var_list=['TV', 'Radio', 'Social_Media','Sales']
+
+q2=data1[var_list].corr().abs().drop('Sales')['Sales']
+
+round(q2.max(), 4) # 최대값
+
+# 답: 0.9995
+
+# [참고]
+q2.nlargest(1) # 상위 n 개
+q2.argmax() # 최대값이 있는 위치번호
+q2.idxmax() # 최대값이 있는 인덱스명
+
+
+
+#%%
+
+# =============================================================================
+# 3. 매출액을 종속변수, TV, Radio, Social Media의 예산을 독립변수로 하여 회귀분석을
+# 수행하였을 때, 세 개의 독립변수의 회귀계수를 큰 것에서부터 작은 것 순으로
+# 기술하시오. 
+# - 분석 시 결측치가 포함된 행은 제거한 후 진행하며, 회귀계수는 소수점 넷째 자리
+# 이하는 버리고 소수점 셋째 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+from sklearn.linear_model import LinearRegression
+from statsmodels.formula.api import ols
+from statsmodels.api import OLS, add_constant
+
+# LinearRegression
+
+q3=data1.dropna()
+var_list=['TV', 'Radio', 'Social_Media']
+
+
+lm=LinearRegression(fit_intercept=True)
+lm.fit(q3[var_list], q3['Sales'])
+
+dir(lm)
+
+lm.intercept_ # 절편
+lm.coef_ # 회귀계수
+
+# [ 3.56256963, -0.00397039,  0.00496402]
+
+# ols
+
+# lm2=ols(form, data)
+# lm3=lm2.fit()
+# lm2=ols(form, data).fit()
+# form :  'y~x1+C(x2)+x3-1'
+
+form1='Sales~'+'+'.join(var_list)
+print(form1) # Sales~TV+Radio+Social_Media
+
+lm2=ols(form1, q3).fit()
+
+dir(lm2)
+lm2.summary()
+
+# OLS: 절편 미포함 
+# - 절편 구할 수 있도록 1만 들어가 있는 상수항 변수 추가
+
+X=add_constant(q3[var_list])
+lm3=OLS(q3['Sales'], X).fit()
+lm3.summary()
+
+
+lm2.params
+lm2.pvalues[lm2.pvalues < 0.05]
+
+lm2.params.drop('Intercept').sort_values(ascending=False)
+
+# 답: 
+# TV              3.562570
+# Social_Media    0.004964
+# Radio          -0.003970
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 02 유형(DataSet_02.csv 이용)
+# 구분자 : comma(“,”), 200 Rows, 6 Columns, UTF-8 인코딩
+
+# 환자의 상태와 그에 따라 처방된 약에 대한 정보를 분석하고자한다
+# 
+# 컬 럼 / 정 의  / Type
+# Age  / 연령 / Integer
+# Sex / 성별 / String
+# BP / 혈압 레벨 / String
+# Cholesterol / 콜레스테롤 레벨 /  String
+# Na_to_k / 혈액 내 칼륨에 대비한 나트륨 비율 / Double
+# Drug / Drug Type / String
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data2=pd.read_csv('Dataset_02.csv')
+
+
+#%%
+
+# =============================================================================
+# 1.해당 데이터에 대한 EDA를 수행하고, 여성으로 혈압이 High, Cholesterol이 Normal인
+# 환자의 전체에 대비한 비율이 얼마인지 소수점 네 번째 자리에서 반올림하여 소수점 셋째
+# 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+data2.columns
+# ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug']
+
+var_list=['Sex', 'BP', 'Cholesterol']
+q1=data2[var_list].value_counts(normalize=True)
+q1
+
+q1[('F', 'HIGH', 'NORMAL')]
+
+# 답: 0.105
+
+
+#%%
+
+# =============================================================================
+# 2. Age, Sex, BP, Cholesterol 및 Na_to_k 값이 Drug 타입에 영향을 미치는지 확인하기
+# 위하여 아래와 같이 데이터를 변환하고 분석을 수행하시오. 
+# - Age_gr 컬럼을 만들고, Age가 20 미만은 ‘10’, 20부터 30 미만은 ‘20’, 30부터 40 미만은
+# ‘30’, 40부터 50 미만은 ‘40’, 50부터 60 미만은 ‘50’, 60이상은 ‘60’으로 변환하시오. 
+# - Na_K_gr 컬럼을 만들고 Na_to_k 값이 10이하는 ‘Lv1’, 20이하는 ‘Lv2’, 30이하는 ‘Lv3’, 30 
+# 초과는 ‘Lv4’로 변환하시오.
+# - Sex, BP, Cholesterol, Age_gr, Na_K_gr이 Drug 변수와 영향이 있는지 독립성 검정을
+# 수행하시오.
+# - 검정 수행 결과, Drug 타입과 연관성이 있는 변수는 몇 개인가? 연관성이 있는 변수
+# 가운데 가장 큰 p-value를 찾아 소수점 여섯 번째 자리 이하는 버리고 소수점 다섯
+# 번째 자리까지 기술하시오.
+# (답안 예시) 3, 1.23456
+# =============================================================================
+
+q2=data2.copy()
+
+# 변수 생성
+
+q2['Age_gr']=np.where(q2.Age < 20, 10, 
+                np.where(q2.Age < 30, 20,
+                   np.where(q2.Age < 40, 30,
+                      np.where(q2.Age < 50, 40, 
+                         np.where(q2.Age < 60, 50, 60)))))
+
+q2['Na_K_gr']=np.where(q2.Na_to_K <= 10, 'Lv1',
+                 np.where(q2.Na_to_K <= 20, 'Lv2',
+                    np.where(q2.Na_to_K <= 30, 'Lv3', 'Lv4')))      
+
+# 교차표 작성
+q2.columns
+# ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug', 'Age_gr',
+#       'Na_K_gr']
+
+
+temp=pd.crosstab(index=q2['Sex'],
+                 columns=q2['Drug'])
+
+# 카이스퀘어 검정
+from scipy.stats import chi2_contingency
+
+q2_out=chi2_contingency(temp)
+q2_out[1]  # pvalue
+
+# 반복 적용
+from scipy.stats import chi2_contingency
+
+var_list=['Sex', 'BP', 'Cholesterol', 'Age_gr', 'Na_K_gr']
+
+q2_out2=[]
+for i in var_list:
+    temp=pd.crosstab(index=q2[i],
+                     columns=q2['Drug'])
+    q2_out=chi2_contingency(temp)
+    chi2=q2_out[0]
+    pvalue=q2_out[1]
+    q2_out2.append([i, chi2, pvalue])
+
+q2_out2=pd.DataFrame(q2_out2, columns=['var', 'chi2', 'pvalue'])
+
+# 영향력 있는 변수 추출
+q2_out3=q2_out2[q2_out2.pvalue < 0.05]
+len(q2_out3)  # 4
+
+# 영향력 변수 중에 가장 큰 p-value
+q2_out3.pvalue.max() # 0.0007010113024729462
+
+# 답: 4, 0.00070
+
+#%%
+
+# =============================================================================
+# 3.Sex, BP, Cholesterol 등 세 개의 변수를 다음과 같이 변환하고 의사결정나무를 이용한
+# 분석을 수행하시오.
+# - Sex는 M을 0, F를 1로 변환하여 Sex_cd 변수 생성
+# - BP는 LOW는 0, NORMAL은 1 그리고 HIGH는 2로 변환하여 BP_cd 변수 생성
+# - Cholesterol은 NORMAL은 0, HIGH는 1로 변환하여 Ch_cd 생성
+# - Age, Na_to_k, Sex_cd, BP_cd, Ch_cd를 Feature로, Drug을 Label로 하여 의사결정나무를
+# 수행하고 Root Node의 split feature와 split value를 기술하시오. 
+# 이 때 split value는 소수점 셋째 자리까지 반올림하여 기술하시오. (답안 예시) Age, 
+# 12.345
+# =============================================================================
+
+q3=data2.copy()
+
+# 변수 생성
+q3['Sex_cd']=np.where(q3.Sex == 'M', 0, 1)
+q3['BP_cd']=np.where(q3.BP == 'LOW', 0,
+                np.where(q3.BP == 'NORMAL', 1, 2))
+q3['Ch_cd']=np.where(q3.Cholesterol == 'NORMAL', 0, 1)
+
+
+# 의사결정나무 실행 -> 모델 생성
+from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
+
+var_list=['Age', 'Na_to_K', 'Sex_cd', 'BP_cd', 'Ch_cd']
+
+dt=DecisionTreeClassifier().fit(q3[var_list], q3['Drug'])
+
+
+# Root Node의 split feature와 split value
+
+
+plot_tree(dt, max_depth=1, feature_names=var_list,
+          class_names=list(q3.Drug.unique()),
+          precision=3,
+          fontsize=8)
+
+
+print(export_text(dt, feature_names=var_list, decimals=3))
+
+# 답: Na_to_K , 14.829
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 03 유형(DataSet_03.csv 이용)
+# 
+# 구분자 : comma(“,”), 5,001 Rows, 8 Columns, UTF-8 인코딩
+# 안경 체인을 운영하고 있는 한 회사에서 고객 사진을 바탕으로 안경의 사이즈를
+# 맞춤 제작하는 비즈니스를 기획하고 있다. 우선 데이터만으로 고객의 성별을
+# 파악하는 것이 가능할 지를 연구하고자 한다.
+#
+# 컬 럼 / 정 의 / Type
+# long_hair / 머리카락 길이 (0 – 길지 않은 경우 / 1 – 긴
+# 경우) / Integer
+# forehead_width_cm / 이마의 폭 (cm) / Double
+# forehead_height_cm / 이마의 높이 (cm) / Double
+# nose_wide / 코의 넓이 (0 – 넓지 않은 경우 / 1 – 넓은 경우) / Integer
+# nose_long / 코의 길이 (0 – 길지 않은 경우 / 1 – 긴 경우) / Integer
+# lips_thin / 입술이 얇은지 여부 0 – 얇지 않은 경우 / 1 –
+# 얇은 경우) / Integer
+# distance_nose_to_lip_long / 인중의 길이(0 – 인중이 짧은 경우 / 1 – 인중이
+# 긴 경우) / Integer
+# gender / 성별 (Female / Male) / String
+# =============================================================================
+# =============================================================================
+
+
+
+#%%
+
+# =============================================================================
+# 1.이마의 폭(forehead_width_cm)과 높이(forehead_height_cm) 사이의
+# 비율(forehead_ratio)에 대해서 평균으로부터 3 표준편차 밖의 경우를 이상치로
+# 정의할 때, 이상치에 해당하는 데이터는 몇 개인가? (답안 예시) 10
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.성별에 따라 forehead_ratio 평균에 차이가 있는지 적절한 통계 검정을 수행하시오.
+# - 검정은 이분산을 가정하고 수행한다.
+# - 검정통계량의 추정치는 절대값을 취한 후 소수점 셋째 자리까지 반올림하여
+# 기술하시오.
+# - 신뢰수준 99%에서 양측 검정을 수행하고 결과는 귀무가설 기각의 경우 Y로, 그렇지
+# 않을 경우 N으로 답하시오. (답안 예시) 1.234, Y
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.주어진 데이터를 사용하여 성별을 구분할 수 있는지 로지스틱 회귀분석을 적용하여
+# 알아 보고자 한다. 
+# - 데이터를 7대 3으로 나누어 각각 Train과 Test set로 사용한다. 이 때 seed는 123으로
+# 한다.
+# - 원 데이터에 있는 7개의 변수만 Feature로 사용하고 gender를 label로 사용한다.
+# (forehead_ratio는 사용하지 않음)
+# - 로지스틱 회귀분석 예측 함수와 Test dataset를 사용하여 예측을 수행하고 정확도를
+# 평가한다. 이 때 임계값은 0.5를 사용한다. 
+# - Male의 Precision 값을 소수점 둘째 자리까지 반올림하여 기술하시오. (답안 예시) 
+# 0.12
+# 
+# 
+# (참고) 
+# from sklearn.linear_model import LogisticRegression
+# from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+# train_test_split 의 random_state = 123
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 04 유형(DataSet_04.csv 이용)
+#
+#구분자 : comma(“,”), 6,718 Rows, 4 Columns, UTF-8 인코딩
+
+# 한국인의 식생활 변화가 건강에 미치는 영향을 분석하기에 앞서 육류
+# 소비량에 대한 분석을 하려고 한다. 확보한 데이터는 세계 각국의 1인당
+# 육류 소비량 데이터로 아래와 같은 내용을 담고 있다.
+
+# 컬 럼 / 정 의 / Type
+# LOCATION / 국가명 / String
+# SUBJECT / 육류 종류 (BEEF / PIG / POULTRY / SHEEP) / String
+# TIME / 연도 (1990 ~ 2026) / Integer
+# Value / 1인당 육류 소비량 (KG) / Double
+# =============================================================================
+# =============================================================================
+
+# (참고)
+# #1
+# import pandas as pd
+# import numpy as np
+# #2
+# from scipy.stats import ttest_rel
+# #3
+# from sklearn.linear_model import LinearRegression
+
+#%%
+
+# =============================================================================
+# 1.한국인의 1인당 육류 소비량이 해가 갈수록 증가하는 것으로 보여 상관분석을 통하여
+# 확인하려고 한다. 
+# - 데이터 파일로부터 한국 데이터만 추출한다. 한국은 KOR로 표기되어 있다.
+# - 년도별 육류 소비량 합계를 구하여 TIME과 Value간의 상관분석을 수행하고
+# 상관계수를 소수점 셋째 자리에서 반올림하여 소수점 둘째 자리까지만 기술하시오. 
+# (답안 예시) 0.55
+# =============================================================================
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2. 한국 인근 국가 가운데 식생의 유사성이 상대적으로 높은 일본(JPN)과 비교하여, 연도별
+# 소비량에 평균 차이가 있는지 분석하고자 한다.
+# - 두 국가의 육류별 소비량을 연도기준으로 비교하는 대응표본 t 검정을 수행하시오.
+# - 두 국가 간의 연도별 소비량 차이가 없는 것으로 판단할 수 있는 육류 종류를 모두
+# 적으시오. (알파벳 순서) (답안 예시) BEEF, PIG, POULTRY, SHEEP
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.(한국만 포함한 데이터에서) Time을 독립변수로, Value를 종속변수로 하여 육류
+# 종류(SUBJECT) 별로 회귀분석을 수행하였을 때, 가장 높은 결정계수를 가진 모델의
+# 학습오차 중 MAPE를 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 21.12
+# (MAPE : Mean Absolute Percentage Error, 평균 절대 백분율 오차)
+# (MAPE = Σ ( | y - y ̂ | / y ) * 100/n ))
+# 
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 05 유형(DataSet_05.csv 이용)
+#
+# 구분자 : comma(“,”), 8,068 Rows, 12 Columns, UTF-8 인코딩
+#
+# A자동차 회사는 신규 진입하는 시장에 기존 모델을 판매하기 위한 마케팅 전략을 
+# 세우려고 한다. 기존 시장과 고객 특성이 유사하다는 전제 하에 기존 고객을 세분화하여
+# 각 그룹의 특징을 파악하고, 이를 이용하여 신규 진입 시장의 마케팅 계획을 
+# 수립하고자 한다. 다음은 기존 시장 고객에 대한 데이터이다.
+#
+
+# 컬 럼 / 정 의 / Type
+# ID / 고유 식별자 / Double
+# Age / 나이 / Double
+# Age_gr / 나이 그룹 (10/20/30/40/50/60/70) / Double
+# Gender / 성별 (여성 : 0 / 남성 : 1) / Double
+# Work_Experience / 취업 연수 (0 ~ 14) / Double
+# Family_Size / 가족 규모 (1 ~ 9) / Double
+# Ever_Married / 결혼 여부 (Unknown : 0 / No : 1 / Yes : 2) / Double
+# Graduated / 재학 중인지 여부 / Double
+# Profession / 직업 (Unknown : 0 / Artist ~ Marketing 등 9개) / Double
+# Spending_Score / 소비 점수 (Average : 0 / High : 1 / Low : 2) / Double
+# Var_1 / 내용이 알려지지 않은 고객 분류 코드 (0 ~ 7) / Double
+# Segmentation / 고객 세분화 결과 (A ~ D) / String
+# =============================================================================
+# =============================================================================
+
+
+#(참고)
+#1
+# import pandas as pd
+# #2
+# from scipy.stats import chi2_contingency
+# #3
+# from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+# from sklearn.tree import DecisionTreeClassifier
+# from sklearn.tree import export_graphviz
+# import pydot
+
+
+#%%
+
+# =============================================================================
+# 1.위의 표에 표시된 데이터 타입에 맞도록 전처리를 수행하였을 때, 데이터 파일 내에
+# 존재하는 결측값은 모두 몇 개인가? 숫자형 데이터와 문자열 데이터의 결측값을
+# 모두 더하여 답하시오.
+# (String 타입 변수의 경우 White Space(Blank)를 결측으로 처리한다) (답안 예시) 123
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.이어지는 분석을 위해 결측값을 모두 삭제한다. 그리고, 성별이 세분화(Segmentation)에
+# 영향을 미치는지 독립성 검정을 수행한다. 수행 결과, p-value를 반올림하여 소수점
+# 넷째 자리까지 쓰고, 귀무가설을 기각하면 Y로, 기각할 수 없으면 N으로 기술하시오. 
+# (답안 예시) 0.2345, N
+# =============================================================================
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.Segmentation 값이 A 또는 D인 데이터만 사용하여 의사결정 나무 기법으로 분류
+# 정확도를
+# 측정해 본다. 
+# - 결측치가 포함된 행은 제거한 후 진행하시오.
+# - Train대 Test 7대3으로 데이터를 분리한다. (Seed = 123)
+# - Train 데이터를 사용하여 의사결정나무 학습을 수행하고, Test 데이터로 평가를
+# 수행한다.
+# - 의사결정나무 학습 시, 다음과 같이 설정하시오:
+# • Feature: Age_gr, Gender, Work_Experience, Family_Size, 
+#             Ever_Married, Graduated, Spending_Score
+# • Label : Segmentation
+# • Parameter : Gini / Max Depth = 7 / Seed = 123
+# 이 때 전체 정확도(Accuracy)를 소수점 셋째 자리 이하는 버리고 소수점 둘째자리까지
+# 기술하시오.
+# (답안 예시) 0.12
+# =============================================================================
+
+
+
Index: ProDS/Set06_Set10_수정.py
===================================================================
diff --git a/ProDS/Set06_Set10_수정.py b/ProDS/Set06_Set10_수정.py
new file mode 100644
--- /dev/null	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
+++ b/ProDS/Set06_Set10_수정.py	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
@@ -0,0 +1,674 @@
+# -*- coding: utf-8 -*-
+"""
+Created on 2021
+
+@author: Administrator
+"""
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 06 유형(DataSet_06.csv 이용)
+#
+# 구분자 : comma(“,”), 4,323 Rows, 19 Columns, UTF-8 인코딩
+
+# 주택 관련 정보를 바탕으로 주택 가격을 예측해 보고자 한다. 
+# 다음은 확보한 주택 관련 데이터로 총 19개 컬럼으로 구성되어
+# 있다.
+
+# 컬 럼 / 정 의 / Type
+# id / 매물 번호 / Double
+# date / 날짜 / String
+# price / 거래 가격 / Double
+# bedrooms / 방 개수 / Double
+# bathrooms / 화장실 개수 (화장실은 있으나 샤워기 없는 경우 0.5로 처리) / Double
+# sqft_living / 건축물 면적 / Double
+# sqft_lot / 대지 면적 / Double
+# floors / 건축물의 층수 / Double
+# waterfront / 강변 조망 가능 여부 (0 / 1) / Double
+# view / 경관 (나쁨에서 좋음으로 0 ~ 4로 표시) / Double
+# condition / 관리 상태 (나쁨에서 좋음으로 1 ~ 5로 표시) / Double
+# grade / 등급 (낮음에서 높음으로 1 ~ 13으로 표시) / Double
+# sqft_above / 지상 면적 / Double
+# sqft_basement / 지하실 면적 / Double
+# yr_built / 건축 연도 / Double
+# yr_renovated / 개축 연도 / Double
+# zipcode / 우편번호 / Double
+# sqft_living15 / 15개의 인근 주택의 평균 건물 면적 / Double
+# sqft_lot15 / 15개의 인근 주택의 평균 대지 면적 / Double
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data6=pd.read_csv('Dataset_06.csv', na_values=['?', ' ', 'NA'])
+data6.info()
+data6.columns
+
+#%%
+
+# =============================================================================
+# 1.강변 조망이 가능한지 여부(waterfront)에 따라 평균 주택 가격을 계산하고 조망이
+# 가능한 경우와 그렇지 않은 경우의 평균 가격 차이의 절대값을 구하시오. 답은
+# 소수점 이하는 버리고 정수부만 기술하시오. (답안 예시) 1234567
+# =============================================================================
+
+
+q1_1=data6[data6.waterfront == 1]['price'].mean()
+q1_0=data6[data6.waterfront == 0]['price'].mean()
+
+
+abs(q1_1 - q1_0)
+
+# 답: 1167272
+
+#%%
+
+# =============================================================================
+# 2.price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, yr_built 등 
+# 7개의 변수 간의
+# 상관분석을 수행하고 price와의 상관계수의 절대값이 가장 큰 변수와 가장 작은
+# 변수를 차례로 기술하시오. (답안 예시) view, zipcode
+# 
+# =============================================================================
+
+var_list=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 
+          'floors', 'yr_built']
+
+q2=data6[var_list].corr().drop('price')['price']
+
+abs(q2).idxmax() # 'sqft_living'
+abs(q2).idxmin() # 'yr_built'
+
+# 답: 'sqft_living' , 'yr_built'
+
+#%%
+
+# =============================================================================
+# 3. id, date, 그리고 zipcode를 제외한 모든 변수를 독립변수로, price를 종속변수로 하여
+# 회귀분석을 수행하시오. 통계적 유의성을 갖지 못하는 독립변수를 제거하면 회귀
+# 모형에 남는 변수는 모두
+# 몇 개인가? 이 때 음의 회귀계수를 가지는 변수는 몇 개인가? (답안 예시) 5, 3
+# =============================================================================
+
+# =============================================================================
+# (참고)
+# import pandas as pd
+# import numpy as np
+# from sklearn.linear_model import LinearRegression
+# from statsmodels.formula.api import ols
+# =============================================================================
+
+var_list=data6.columns.drop(['id', 'date', 'zipcode', 'price'])
+
+from statsmodels.formula.api import ols
+
+form1='price~'+'+'.join(var_list)
+
+ols1=ols(form1, data6).fit()
+
+# 각 독립변수별 유의성 검정
+q3_out=ols1.pvalues.drop('Intercept')
+(q3_out < 0.05).sum() # 13
+
+# q3_out[q3_out < 0.05]
+
+q3_sel_list=q3_out.index[q3_out < 0.05]
+
+# 유의한 변수 중 음수인 회귀계수 수
+(ols1.params[q3_sel_list] < 0).sum()  # 2
+
+# (ols1.params < 0).sum() # 전체 대상
+
+# 답: 13, 2
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 07 유형(DataSet_07.csv 이용)
+#
+# 구분자 : comma(“,”), 400 Rows, 9 Columns, UTF-8 인코딩
+#
+# 대학원 진학을 위하여 어떤 항목이 중요하게 영향을 미치는지
+# 아래 데이터로 분석하고자 한다.
+
+# 컬 럼 / 정 의 / Type
+# Serial_No / 구분자 번호 / Double
+# GRE / GRE 시험 성적 / Double
+# TOEFL / TOEFL 시험 성적 / Double
+# University_Rating / 대학 평가 그룹 (1 ~ 5) / Double
+# SOP / 자기 소개서 점수 (1 ~ 5) / Double
+# LOR / 추천서 점수 (1 ~ 5) / Double
+# CGPA / 학부 평량 평점 (10점 만점 환산 점수) / Double
+# Research / 연구 참여 경험 여부 (0 / 1) / Double
+# Chance_of_Admit / 합격 가능성 / Double
+# =============================================================================
+# =============================================================================
+
+# =============================================================================
+# (참고)
+# #1
+# import pandas as pd
+# #2
+# import scipy.stats as stats
+# #3
+# from sklearn.linear_model import LogisticRegression
+# Solver = ‘liblinear’, random_state = 12
+# =============================================================================
+
+
+#%%
+
+import pandas as pd
+import numpy as np
+
+data7=pd.read_csv("Dataset_07.csv")
+data7.columns
+# ['Serial_No', 'GRE', 'TOEFL', 'University_Rating', 'SOP', 'LOR', 'CGPA',
+#        'Research', 'Chance_of_Admit']
+# =============================================================================
+# 1. 합격 가능성에 GRE, TOEFL, CGPA 점수 가운데 가장 영향이 큰 것이 어떤 점수인지
+# 알아 보기 위해서 상관 분석을 수행한다.
+# - 피어슨(Pearson) 상관계수 값을 구한다.
+# - Chance_of_Admit와의 가장 큰 상관계수 값을 가지는 항목의 상관계수를 소수점 넷째
+# 자리에서 반올림하여 셋째 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+
+var_list=['GRE', 'TOEFL', 'CGPA','Chance_of_Admit']
+
+
+q1=data7[var_list].corr().drop('Chance_of_Admit')['Chance_of_Admit']
+abs(q1).max() 
+
+# 답: 0.8732890993553003 -> 0.873
+
+#%%
+
+# =============================================================================
+# 2.GRE 점수의 평균 이상을 받은 그룹과 평균 미만을 받은 그룹의 CGPA 평균은 차이가
+# 있는지
+# 검정을 하고자 한다.
+# - 적절한 검정 방법을 선택하고 양측 검정을 수행하시오 (등분산으로 가정)
+# - 검정 결과, 검정통계량의 추정치를 소수점 셋째 자리에서 반올림하여 소수점 두 자리까지
+# 기술하시오.
+# (답안 예시) 1.23
+# =============================================================================
+
+mu=data7['GRE'].mean()
+
+q2=data7.copy()
+q2['GRE_gr']=np.where(q2.GRE >= mu, 1, 0)
+
+g1=q2[q2.GRE_gr == 1]['CGPA']
+g0=q2[q2.GRE_gr == 0]['CGPA']
+
+
+from scipy.stats import ttest_ind
+
+q2_out=ttest_ind(g1, g0, equal_var=True)
+
+q2_out.statistic
+
+#답:  19.443291692470982 -> 19.44
+
+#%%
+
+# =============================================================================
+# 3.Chance_of_Admit 확률이 0.5를 초과하면 합격으로, 이하이면 불합격으로 구분하고
+# 로지스틱 회귀분석을 수행하시오.
+# - 원데이터만 사용하고, 원데이터 가운데 Serial_No와 Label은 모형에서 제외
+# - 각 설정값은 다음과 같이 지정하고, 언급되지 않은 사항은 기본 설정값을 사용하시오
+# Seed : 123
+# - 로지스틱 회귀분석 수행 결과에서 로지스틱 회귀계수의 절대값이 가장 큰 변수와 그 값을
+# 기술하시오. 
+# (로지스틱 회귀계수는 반올림하여 소수점 둘째 자리까지 / Intercept는 제외)
+# (답안 예시) abc, 0.12
+# =============================================================================
+
+q3=data7.copy()
+
+q3['Ch_cd']=np.where(q3.Chance_of_Admit > 0.5 , 1, 0)
+
+from sklearn.linear_model import LogisticRegression
+
+x_list=data7.columns.drop(['Serial_No','Chance_of_Admit'])
+
+logit=LogisticRegression(fit_intercept=False, 
+                         random_state=12, solver = 'liblinear')
+
+logit.fit(q3[x_list], q3['Ch_cd'])
+#abs(logit.coef_).max() # 1.955355062462584
+
+logit.coef_.shape
+q3_out=pd.Series(logit.coef_.reshape(-1))
+
+q3_out.index=x_list
+q3_out.abs().nlargest(1)
+
+# 답: CGPA, 1.955355 -> CGPA,    1.96
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 08 유형(DataSet_08.csv 이용)
+#
+# 구분자 : comma(“,”), 50 Rows, 5 Columns, UTF-8 인코딩
+#
+# 스타트업 기업들의 수익성에 대한 분석을 하기 위하여
+# 아래와 같은 데이터를 입수하였다
+#
+# 
+# 컬 럼 / 정 의 / Type
+# RandD_Spend / 연구개발비 지출 / Double
+# Administration / 운영관리비 지출 / Double
+# Marketing_Spend / 마케팅비 지출 / Double
+# State / 본사 위치 / String
+# Profit / 이익 / Double
+# =============================================================================
+# =============================================================================
+
+# =============================================================================
+# (참고)
+# #1
+# import pandas as pd
+# import numpy as np
+# #3
+# from sklearn.linear_model import LinearRegression
+# =============================================================================
+
+#%%
+
+import pandas as pd
+import numpy as np
+
+data8=pd.read_csv('Dataset_08.csv')
+data8.columns
+# ['RandD_Spend', 'Administration', 'Marketing_Spend', 'State', 'Profit']
+# =============================================================================
+# 1.각 주(State)별 데이터 구성비를 소수점 둘째 자리까지 구하고, 알파벳 순으로
+# 기술하시오(주 이름 기준).
+# (답안 예시) 0.12, 0.34, 0.54
+# =============================================================================
+
+
+data8['State'].value_counts(normalize=True).sort_index().values
+
+# 답: 0.34, 0.32, 0.34]
+
+#%%
+
+# =============================================================================
+# 2.주별 이익의 평균을 구하고, 평균 이익이 가장 큰 주와 작은 주의 차이를 구하시오. 
+# 차이값은 소수점 이하는 버리고 정수부분만 기술하시오. (답안 예시) 1234
+# =============================================================================
+
+
+q2=data8.groupby('State')['Profit'].mean()
+
+q2.max() - q2.min()
+
+# 답: 14868
+
+#%%
+
+# =============================================================================
+# 3.독립변수로 RandD_Spend, Administration, Marketing_Spend를 사용하여 Profit을 주별로
+# 예측하는 회귀 모형을 만들고, 이 회귀모형을 사용하여 학습오차를 산출하시오.
+# - 주별로 계산된 학습오차 중 MAPE 기준으로 가장 낮은 오차를 보이는 주는 어느
+# 주이고 그 값은 무엇인가? (반올림하여 소수점 둘째 자리까지 기술하시오)
+# - (MAPE = Σ ( | y - y ̂ | / y ) * 100/n )
+# (답안 예시) ABC, 1.56
+# =============================================================================
+
+
+state_list=data8.State.unique()
+# ['New York', 'California', 'Florida']
+var_list=['RandD_Spend', 'Administration', 'Marketing_Spend']
+
+from sklearn.linear_model import LinearRegression
+
+q3_out=[]
+
+for i in state_list:
+    temp=data8[data8.State == i]
+    lm=LinearRegression().fit(temp[var_list], temp['Profit'])
+    pred=lm.predict(temp[var_list])
+    mape=(abs(temp['Profit'] - pred) / temp['Profit']).sum() * 100 / len(temp)
+    q3_out.append([i, mape])
+
+q3_out=pd.DataFrame(q3_out, columns=['state','mape'])
+
+q3_out.loc[q3_out.mape.idxmin(),:]
+
+# 답: Florida, 5.71
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 09 유형(DataSet_09.csv 이용)
+#
+# 구분자 : comma(“,”), 2000 Rows, 16 Columns, UTF-8 인코딩
+#
+# 항공사에서 고객만족도 조사를 하고 서비스 개선에 활용하고자
+# 아래와 같은 데이터를 준비하였다.
+#
+# 컬 럼 / 정 의 / Type
+# satisfaction / 서비스 만족 여부 / String
+# Gender / 성별 / String
+# Age / 나이 / Double
+# Customer_Type / 고객 타입 / String
+# Class / 탑승 좌석 등급 / String
+# Flight_Distance / 비행 거리 / Double
+# Seat_comfort / 좌석 안락도 점수 / Double
+# Food_and_Drink / 식사와 음료 점수 / Double
+# Inflight_wifi_service / 기내 와이파이 서비스 점수 / Double
+# Inflight_entertainment / 기내 엔터테인먼트 서비스 점수 / Double
+# Onboard_service / 탑승 서비스 점수 / Double
+# Leg_room_service / 다리 공간 점수 / Double
+# Baggage_handling / 수하물 취급 점수 / Double
+# Cleanliness / 청결도 점수 / Double
+# Departure_Daly_in_Minutes / 출발 지연 (분) / Double
+# Arrival_Delay_in_Minutes / 도착 지연 (분) / Double
+# =============================================================================
+# =============================================================================
+
+# =============================================================================
+# (참고)
+# #1
+# import pandas as pd
+# import numpy as np
+# #2
+# import scipy.stats as stats
+# #3
+# from sklearn.linear_model import LogisticRegression
+# from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+# 
+# =============================================================================
+
+#%%
+
+import pandas as pd
+
+data9=pd.read_csv('Dataset_09.csv')
+
+#%%
+
+# =============================================================================
+# 1.데이터 타입을 위 표에 정의된 타입으로 전처리를 한 후, 데이터 파일 내에 결측값은
+# 총 몇 개인가? (답안 예시) 1
+# =============================================================================
+
+
+data9.isna().sum().sum()
+
+
+# (정답) 5
+
+#%%
+# =============================================================================
+# 2.다음에 제시된 데이터 처리를 하고 카이제곱 독립성 검정을 수행하시오.
+# - 결측값이 있다면 해당 행을 제거하시오.
+# - 나이는 20 이하이면 10, 30 이하이면 20, 40 이하이면 30, 50 이하이면 40, 60 이하이면 50, 
+# 60 초과는 60으로 변환하여 Age_gr으로 파생변수를 생성하시오.
+# - Age_gr, Gender, Customer_Type, Class 변수가 satisfaction에 영향이 있는지 카이제곱
+# 독립성 검정을 수행하시오. 
+# - 연관성이 있는 것으로 파악된 변수의 검정통계량 추정치를 정수 부분만 기술하시오. 
+# (답안 예시) 123
+# =============================================================================
+
+q2=data9.copy()
+q2.columns
+
+import numpy as np
+q2['Age_gr']=np.where(q2.Age <= 20, 10,
+                np.where(q2.Age <= 30, 20,
+                   np.where(q2.Age <= 40, 30,
+                     np.where(q2.Age <= 50, 40,
+                        np.where(q2.Age <= 60, 50,  60)))))
+
+
+from scipy.stats import chi2_contingency
+
+
+# - Age_gr, Gender, Customer_Type, Class 변수가 satisfaction에 영향이 있는지
+var_list=['Age_gr', 'Gender', 'Customer_Type', 'Class']
+
+q2_out=[]
+for i in var_list:
+    q2_tab=pd.crosstab(index=q2[i], columns=q2['satisfaction'])
+    chi, pvalue, *_=chi2_contingency(q2_tab)
+    q2_out.append([i, chi, pvalue])
+
+q2_out=pd.DataFrame(q2_out, columns=['var', 'chi', 'pvalue'])
+
+q2_out[q2_out.pvalue < 0.05]['chi']
+
+# (정답) 1068.632582 -> 1068
+
+#%%
+
+# =============================================================================
+# 3.고객 만족도를 라벨로 하여 다음과 같이 로지스틱 회귀분석을 수행하시오. 
+# - 결측치가 포함된 행은 제거
+# - 데이터를 7대 3으로 분리 (Seed = 123)
+# - 아래의 11개 변수를 Feature로 사용
+# Flight_Distance, Seat_comfort, Food_and_drink, Inflight_wifi_service, 
+# Inflight_entertainment,Onboard_service, Leg_room_service, Baggage_handling,
+# Cleanliness, Departure_Delay_in_Minutes, Arrival_Delay_in_Minutes
+# 
+# - Seed = 123, 이외의 항목은 모두 Default 사용
+# - 예측 정확도를 측정하고 dissatisfied의 f1 score를 소수점 넷째 자리에서 반올림하여
+# 소수점 셋째 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+
+
+from sklearn.linear_model import LogisticRegression
+from sklearn.model_selection import train_test_split
+# from sklearn import metrics
+from sklearn.metrics import f1_score, classification_report
+
+
+x_var=['Flight_Distance', 'Seat_comfort', 'Food_and_drink',
+       'Inflight_wifi_service','Inflight_entertainment', 'Onboard_service', 
+       'Leg_room_service', 'Baggage_handling', 'Cleanliness',
+       'Departure_Delay_in_Minutes', 'Arrival_Delay_in_Minutes']
+
+q3=data9.dropna()
+
+X_train, X_test, y_train, y_test = \
+    train_test_split(q3[x_var], q3['satisfaction'], 
+                 test_size = 0.3, random_state = 123)
+
+
+model = LogisticRegression(solver = 'liblinear',random_state=123)
+result = model.fit(X_train, y_train)
+
+y_pred = model.predict(X_test)
+
+
+f1_score(y_test, y_pred, pos_label='dissatisfied')
+
+
+# =============================================================================
+# metrics.accuracy_score(y_test, y_pred)
+# metrics.precision_score(y_test, y_pred)
+
+# metrics.confusion_matrix(y_test, y_pred)
+# FF, FM, MF, MM = metrics.confusion_matrix(y_test, y_pred).ravel()
+# 
+# f1_dissatisfied = 2 / ((1/(258/(79+258))) + (1/(258/(69+258))))
+# f1_dissatisfied
+# 
+# f1_satisfied = 2 / ((1/(193/(79+193))) + (1/(193/(69+193))))
+# f1_satisfied
+# 
+# result.coef_
+# result.get_params()
+# =============================================================================
+
+
+# (정답) 0.777
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 10 유형(DataSet_10.csv 이용)
+#
+# 구분자 : comma(“,”), 1538 Rows, 6 Columns, UTF-8 인코딩
+
+# 중고 자동차 가격에 대한 분석을 위하여 아래와 같은 데이터를
+# 확보하였다.
+
+# 컬 럼 / 정 의 / Type
+# model / 모델명 / String
+# engine_power / 엔진 파워 / Double
+# age_in_days / 운행 일수 / Double
+# km / 운행 거리 / Double
+# previous_owners / 이전 소유자 수 / Double
+# price / 중고차 가격 / Double
+# =============================================================================
+# =============================================================================
+
+# =============================================================================
+# (참고)
+# #1
+# import pandas as pd
+# import numpy as np
+# #2
+# import scipy.stats as ststs
+# #3
+# from sklearn.linear_model import LinearRegression
+# =============================================================================
+
+#%%
+
+import pandas as pd
+import numpy as np
+
+data10=pd.read_csv('Dataset_10.csv')
+
+data10=data10.dropna(how='all', axis=1)
+
+#%%
+
+# =============================================================================
+# 1.이전 소유자 수가 한 명이고 엔진 파워가 51인 차에 대해 모델별 하루 평균 운행
+# 거리를 산출하였을 때 가장 낮은 값을 가진 모델이 가장 큰 값을 가진 모델에 대한
+# 비율은 얼마인가? 소수점 셋째 자리에서 반올림하여 소수점 둘째 자리까지
+# 기술하시오.
+# (모델별 평균 → 일평균 → 최대최소 비율 계산) (답안 예시) 0.12
+# =============================================================================
+data10.columns
+# ['model', 'engine_power', 'age_in_days', 'km', 'previous_owners', 'price']
+
+
+# 1. 이전 소유자 수가 한 명이고 엔진 파워가 51인 차에 대해
+
+q1=data10[(data10.previous_owners == 1) & (data10.engine_power == 51)]
+
+len(data10)
+len(q1)
+
+# 2. 모델별 하루 평균 운행거리를 산출
+# (1) 모델별 평균
+q1_tab=q1.groupby('model')[['age_in_days', 'km']].mean()
+
+# (2) 일평균
+q1_tab['km_per_day']=q1_tab['km']/q1_tab['age_in_days']
+
+# (3) 최대최소 비율 계산
+q1_tab['km_per_day'].min() / q1_tab['km_per_day'].max()
+
+# 답: 0.97
+
+#%%
+
+# =============================================================================
+# 2.운행 일수에 대한 운행 거리를 산출하고, 위 1번 문제에서 가장 큰 값을 가지고 있던
+# 모델과 가장 낮은 값을 가지고 있던 모델 간의 운행 일수 대비 운행거리 평균이 다른지
+# 적절한 통계 검정을 수행하고 p-value를 소수점 세자리 이하는 버리고 소수점
+# 두자리까지 기술하고 기각 여부를 Y / N로 답하시오. (등분산을 가정하고 equal_var = 
+# True / var.equal = T로 분석을 실행하시오.)
+# (답안 예시) 0.23, Y
+# =============================================================================
+
+# 1. 운행 일수에 대한 운행 거리를 산출
+
+q2=data10.copy()
+
+q2['km_per_day']=q2['km']/q2['age_in_days']
+
+# 2. 위 1번 문제에서 가장 큰 값을 가지고 있던 모델과 가장 낮은 값을 가지고 있던 모델
+
+max_g=q1_tab['km_per_day'].idxmax()
+min_g=q1_tab['km_per_day'].idxmin()
+
+
+# 모델 간의 운행 일수 대비 운행거리 평균이 다른지
+# 적절한 통계 검정을 수행 : ttest
+
+from scipy.stats import ttest_ind
+
+q2_out=ttest_ind(q2[q2.model == max_g]['km_per_day'],
+          q2[q2.model == min_g]['km_per_day'],
+          equal_var=True)
+
+
+#  p-value를 소수점 세자리 이하는 버리고 소수점
+# 두자리까지 기술하고 기각 여부를 Y / N
+
+q2_out.pvalue
+
+# 답: 0.13
+
+#%%
+
+# =============================================================================
+# 3.독립변수로 engine_power, age_in_days, km를 사용하고 종속변수로 price를 사용하여
+# 모델별 선형회귀분석을 수행하고, 산출된 모형을 사용하여 다음과 같은 조건의
+# 중고차에 대한 가격을 예측하고 예측된 가격을 정수부만 기술하시오.
+# - model : pop / engine_power : 51 / age_in_days : 400 / km : 9500 / previous_owners : 2
+
+# (답안 예시) 12345
+# =============================================================================
+# model = pop이고 이전 소유자수가 2명인 데이터만을 이용하여 회귀모델을 생성하시오.
+
+
+# 1. 이전 소유자수가 2인 데이터 필터링
+q3=data10[data10.previous_owners==2]
+
+# 2. 독립변수 목록 & 모델 목록
+var_list=['engine_power', 'age_in_days', 'km']
+model_list=q3.model.unique()  # ['lounge', 'sport', 'pop']
+
+# 3. 반복적으로 모델 생성
+from sklearn.linear_model import  LinearRegression
+
+for i in model_list:
+    temp=q3[q3.model==i]
+    globals()['lm_'+i] = LinearRegression().fit(temp[var_list], temp['price'])
+
+# 4. 예측: 예측 시 나온 모델명의 모델을 호출해서 사용
+# - model : pop / engine_power : 51 / age_in_days : 400 / km : 9500 / previous_owners : 2
+
+pred=lm_pop.predict([[51, 400, 9500]])
+
+# 답: [10367.53433763]
+
+pred2=lm_pop.predict(
+         pd.DataFrame({'engine_power' : [51],
+                       'age_in_days' : [400],
+                       'km' : [9500]}))
+# 답: [10367.53433763]
+
Index: ProDS/Set11_Set15_수정.py
===================================================================
diff --git a/ProDS/Set11_Set15_수정.py b/ProDS/Set11_Set15_수정.py
new file mode 100644
--- /dev/null	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
+++ b/ProDS/Set11_Set15_수정.py	(revision d249f0f97739eb0b4c65537de013ad3522b5a4a6)
@@ -0,0 +1,523 @@
+# -*- coding: utf-8 -*-
+"""
+Created on 2021
+
+@author: Administrator
+"""
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 11 유형(DataSet_11.csv 이용)
+
+# 구분자 : comma(“,”), 470 Rows, 4 Columns, UTF-8 인코딩
+
+# 세계 각국의 행복지수를 비롯한 여러 정보를 조사한 DS리서치는
+# 취합된 자료의 현황 파악 및 간단한 통계분석을 실시하고자 한다.
+
+# 컬 럼 / 정 의 / Type
+# Country / 국가명 / String
+# Happiness_Rank / 당해 행복점수 순위 / Double
+# Happiness_Score / 행복점수 / Double
+# year / 년도 / Double
+# =============================================================================
+# =============================================================================
+
+import pandas as pd
+import numpy as np
+
+data11=pd.read_csv('Dataset_11.csv')
+
+
+#%%
+
+# =============================================================================
+# 1.분석을 위해 3년 연속 행복지수가 기록된 국가의 데이터를 사용하고자 한다. 
+# 3년 연속 데이터가 기록되지 않은 국가의 개수는?
+# - 국가명 표기가 한 글자라도 다른 경우 다른 국가로 처리하시오.
+# - 3년 연속 데이터가 기록되지 않은 국가 데이터는 제외하고 이를 향후 분석에서
+# 활용하시오.(답안 예시) 1
+# =============================================================================
+data11.columns
+# ['Country', 'Happiness_Rank', 'Happiness_Score', 'year']
+
+# 3년 연속 데이터가 기록되지 않은 국가의 개수
+q1=data11.groupby('Country').apply(len)
+(q1 < 3).sum()
+# 답: 20
+
+# 분석을 위해 3년 연속 행복지수가 기록된 국가의 데이터를 사용
+q1[q1 == 3].index
+
+
+## 방법2 
+q1_1=pd.pivot_table(data11, index='Country', 
+                  columns='year', values='Happiness_Score')
+
+q1_1.isna().any(axis=1).sum()
+# 답: 20
+
+# 3년 연속 기록된 데이터만 필터링
+q1_2=q1_1.dropna()
+
+#%%
+
+# =============================================================================
+# 2.(1번 산출물을 활용하여) 2017년 행복지수와 2015년 행복지수를 활용하여 국가별
+# 행복지수 증감률을 산출하고 행복지수 증감률이 가장 높은 3개 국가를 행복지수가
+# 높은 순서대로 차례대로 기술하시오.
+# 증감률 = (2017년행복지수−2015년행복지수)/2
+# 
+#  (답안 예시) Korea, Japan, China
+# =============================================================================
+
+
+q2=q1_2.copy()
+
+# 증감률 = (2017년행복지수−2015년행복지수)/2
+q2['ratio']=(q2[2017] - q2[2015])/2
+
+# 증감률이 가장 높은 3개 국가를 행복지수가 높은 순서대로 차례대로 기술
+q2['ratio'].nlargest(3).index
+
+# 답: ['Latvia', 'Romania', 'Togo'],
+
+
+#%%
+
+# =============================================================================
+# 3.(1번 산출물을 활용하여) 년도별 행복지수 평균이 유의미하게 차이가 나는지
+# 알아보고자 한다. 
+# 이와 관련하여 적절한 검정을 사용하고 검정통계량을 기술하시오.
+# - 해당 검정의 검정통계량은 자유도가 2인 F 분포를 따른다.
+# - 검정통계량은 소수점 넷째 자리까지 기술한다. (답안 예시) 0.1234
+# =============================================================================
+
+# (참고)
+# from statsmodels.formula.api import ols
+# from statsmodels.stats.anova import anova_lm
+
+from scipy.stats import f_oneway   # 분산분석, ANOVA
+from statsmodels.formula.api import ols
+from statsmodels.stats.anova import anova_lm
+
+q3_1=f_oneway(q1_2[2015], q1_2[2016], q1_2[2017])
+# F_onewayResult(statistic=0.004276725037689305, pvalue=0.9957324489944479)
+
+# 답: 0.0042
+
+dir(q3_1)
+
+
+country_list=q1[q1 == 3].index
+
+
+q3_2=data11[data11.Country.isin(country_list)] 
+q3_2.columns
+anova=ols('Happiness_Score~C(year)', data=q3_2).fit()
+
+q3_out=anova_lm(anova)
+
+q3_out['F'][0]
+# 답: 0.0042
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 12 유형(DataSet_12.csv 이용)
+
+# 구분자 : comma(“,”), 5000 Rows, 7 Columns, UTF-8 인코딩
+
+# 직장인의 독서 실태를 분석하기 위해서 수도권 거주자 5000명을
+# 대상으로 간단한 인적 사항과 연간 독서량 정보를 취합하였다.
+
+# 컬 럼 / 정 의 / Type
+# Age / 나이 / String
+# Gender / 성별(M: 남성) / String
+# Dependent_Count / 부양가족 수 / Double
+# Education_Level / 교육 수준 / String
+# is_Married / 결혼 여부(1: 결혼) / Double
+# Read_Book_per_Year / 연간 독서량(권) / Double
+# Income_Range / 소득 수준에 따른 구간(A < B < C < D < E)이며 X는
+# 정보 누락 / String
+# =============================================================================
+# =============================================================================
+
+
+#%%
+
+# =============================================================================
+# 1.수치형 변수를 대상으로 피어슨 상관분석을 실시하고 연간 독서량과 가장
+# 상관관계가 강한 변수의 상관계수를 기술하시오
+# - 상관계수는 반올림하여 소수점 셋째 자리까지 기술하시오. (답안 예시) 0.123
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.석사 이상(석사 및 박사) 여부에 따라서 연간 독서량 평균이 유의미하게 다른지 가설
+# 검정을 활용하여 알아보고자 한다. 독립 2표본 t검정을 실시했을 때 
+# 유의 확률(pvalue)의 값을 기술하시오.
+# - 등분산 가정 하에서 검정을 실시한다.
+# - 유의 확률은 반올림하여 소수점 셋째 자리까지 기술한다. (답안 예시) 0.123
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.독서량과 다른 수치형 변수의 관계를 다중선형회귀분석을 활용하여 알아보고자 한다. 
+# 연간 독서량을 종속변수, 나머지 수치형 자료를 독립변수로 한다. 이렇게 생성한
+# 선형회귀 모델을 기준으로 다른 독립변수가 고정이면서 나이만 다를 때, 40살은 30살
+# 보다 독서량이 얼마나 많은가?
+# - 학사 이상이면서 소득 구간 정보가 있는 데이터만 사용하여 분석을 실시하시오.
+# - 결과값은 반올림하여 정수로 표기하시오. (답안 예시) 1
+# =============================================================================
+
+# (참고)
+# from statsmodels.formula.api import ols
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 13 유형(DataSet13_train.csv / DataSet13_test.csv  이용)
+
+# 구분자 : 
+#     comma(“,”), 1500 Rows, 10 Columns, UTF-8 인코딩 / 
+#     comma(“,”), 500 Rows, 10 Columns, UTF-8 인코딩
+
+# 전국의 데이터 분석가 2000명을 대상으로 이직 관련 설문조사를 실시하였다. 
+# 설문 대상자의 특성 및 이직 의사와 관련 인자를 면밀히 살펴보기 위해 다양한
+# 분석을 실시하고자 한다.
+
+# 컬 럼 / 정 의 / Type
+# city_development_index / 거주 도시 개발 지수 / Double
+# gender / 성별 / String
+# relevent_experience / 관련 직무 경험 여부(1 : 유경험) / Integer
+# enrolled_university / 대학 등록 형태(1 : 풀타임/파트타임) / Integer
+# education_level / 교육 수준 / String
+# major_discipline / 전공 / String
+# experience / 경력 / Double
+# last_new_job / 현 직장 직전 직무 공백 기간 / Double
+# training_hours / 관련 직무 교육 이수 시간 / Double
+# target / 이직 의사 여부(1 : 의사 있음) / Integer
+# =============================================================================
+# =============================================================================
+
+
+#%%
+
+# =============================================================================
+# 1.(Dataset_13_train.csv를 활용하여) 경력과 최근 이직시 공백기간의 상관관계를 보고자
+# 한다. 남여별 피어슨 상관계수를 각각 산출하고 더 높은 상관계수를 기술하시오.
+# - 상관계수는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# =============================================================================
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.(Dataset_13_train.csv를 활용하여) 기존 데이터 분석 관련 직무 경험과 이직 의사가 서로
+# 관련이 있는지 알아보고자 한다. 이를 위해 독립성 검정을 실시하고 해당 검정의 p-value를 기술하시오.
+# - 검정은 STEM 전공자를 대상으로 한다.
+# - 검정은 충분히 발달된 도시(도시 개발 지수가 제 85 백분위수 초과)에 거주하는 사람을
+# 대상으로 한다.
+# - 이직 의사 여부(target)은 문자열로 변경 후 사용한다.
+# - p-value는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+#%%
+
+
+# =============================================================================
+# 3.(Dataset_13_train.csv를 활용하여) 인사팀에서는 어떤 직원이 이직 의사를 가지고 있을지
+# 사전에 파악하고 1:1 면담 등 집중 케어를 하고자 한다. 이를 위해 의사결정 나무를
+# 활용하여 모델을 생성하고 그 정확도를 확인하시오.
+# - target을 종속변수로 하고 나머지 변수 중 String이 아닌 변수를 독립변수로 한다.
+# - 학습은 전부 기본값으로 실시한다.
+# - 평가는 "Dataset_13_test.csv" 데이터로 실시한다.
+# - 정확도는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# 
+# =============================================================================
+
+# (참고)
+# from sklearn.tree import DecisionTreeClassifier
+# random_state = 123
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# =============================================================================
+# # 문제 14 유형(DataSet_14.csv 이용)
+#
+# 구분자 : comma(“,”), 2000 Rows, 9 Columns, UTF-8 인코딩
+#
+# 온라인 교육업체 싱글캠퍼스에서 런칭한 교육 플랫폼을 보다
+# 체계적으로 운영하기 위해 2014년부터 2016년 동안 개설된 강좌
+# 2000개를 대상으로 강좌 실적 및 고객의 서비스 분석을 실시하려고
+# 한다. 관련 데이터는 다음과 같다.
+#
+# 컬 럼 / 정 의 / Type
+# id / 강좌 일련번호 / Double
+# published / 강과 개설일 / String
+# subject / 강좌 대주제 / String
+# level / 난이도 / String
+# price / 가격(만원) / Double
+# subscribers / 구독자 수(결제 인원) / Double
+# reviews / 리뷰 개수 / Double
+# lectures / 강좌 영상 수 / Double
+# duration / 강좌 총 길이(시간) / Double
+# =============================================================================
+# =============================================================================
+
+
+#%%
+
+# =============================================================================
+# 1.결제 금액이 1억 이상이면서 구독자의 리뷰 작성 비율이 10% 이상인 교육의 수는?
+# - 결제 금액은 강좌 가격에 구독자 수를 곱한 값이다.
+# - 리뷰 작성 비율은 리뷰 개수에 구독자 수를 나눈 값이다. (답안 예시) 1
+# =============================================================================
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2.강좌 가격이 비쌀수록 구독자 숫자는 줄어든다는 가설을 확인하기 위해 상관분석을
+# 실시하고자 한다. 2016년 개설된 Web Development 강좌를 대상으로 강좌 가격과
+# 구독자 수의 피어슨 상관관계를 기술하시오.
+# - 상관계수는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.유저가 서비스 사용에 익숙해지고 컨텐츠의 좋은 내용을 서로 공유하려는 경향이
+# 전반적으로 증가하는 추세라고 한다. 이를 위해 먼저 강좌 개설 년도별 구독자의 리뷰
+# 작성 비율의 평균이 강좌 개설 년도별로 차이가 있는지 일원 분산 분석을 통해서
+# 알아보고자 한다. 이 때 검정통계량을 기술하시오.
+# - 검정통계량은 반올림하여 소수점 첫째 자리까지 기술하시오. (답안 예시) 0.1
+#
+# (참고)
+# from statsmodels.formula.api import ols
+# from statsmodels.stats.anova import anova_lm
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+
+# =============================================================================
+# =============================================================================
+# # 문제 05 유형(Dataset_05_Mart_POS.csv /  이용)
+#
+# =============================================================================
+# Dataset_05_Mart_POS.csv 
+# 구분자 : comma(“,”), 20488 Rows, 3 Columns, UTF-8 인코딩
+# =============================================================================
+#
+# 원룸촌에 위치한 A마트는 데이터 분석을 통해 보다 체계적인 재고관리와
+# 운영을 하고자 한다. 이를 위해 다음의 두 데이터 세트를 준비하였다.
+#
+# 컬 럼 / 정 의 / Type
+# Member_number / 고객 고유 번호 / Double
+# Date / 구매일 / String
+# itemDescription / 상품명 / String
+
+# =============================================================================
+# Dataset_05_item_list.csv 
+# 구분자 : comma(“,”), 167 Rows, 4 Columns, UTF-8 인코
+# =============================================================================
+#
+# 컬 럼 / 정 의 / Type
+# prod_id / 상품 고유 번호 / Double
+# prod_nm / 상품명 / String
+# alcohol / 주류 상품 여부(1 : 주류) / Integer
+# frozen / 냉동 상품 여부(1 : 냉동) / Integer
+# =============================================================================
+# =============================================================================
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 1.(Dataset_05_Mart_POS.csv를 활용하여) 가장 많은 제품이 팔린 날짜에 가장 많이 팔린
+# 제품의 판매 개수는? (답안 예시) 1
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 2. (Dataset_05_Mart_POS.csv, Dataset_05_item_list.csv를 활용하여) 고객이 주류 제품을
+# 구매하는 요일이 다른 요일에 비해 금요일과 토요일이 많을 것이라는 가설을 세웠다. 
+# 이를 확인하기 위해 금요일과 토요일의 일별 주류제품 구매 제품 수 평균과 다른
+# 요일의 일별 주류제품 구매 제품 수 평균이 서로 다른지 비교하기 위해 독립 2표본
+# t검정을 실시하시오. 
+# 해당 검정의 p-value를 기술하시오.
+# - 1분기(1월 ~ 3월) 데이터만 사용하여 분석을 실시하시오.
+# - 등분산 가정을 만족하지 않는다는 조건 하에 분석을 실시하시오.
+# - p-value는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#%%
+
+# =============================================================================
+# 3.(Dataset_05_Mart_POS.csv를 활용하여) 1년 동안 가장 많이 판매된 10개 상품을 주력
+# 상품으로 설정하고 특정 요일에 프로모션을 진행할지 말지 결정하고자 한다. 먼저
+# 요일을 선정하기 전에 일원 분산 분석을 통하여 요일별 주력 상품의 판매 개수의
+# 평균이 유의미하게 차이가 나는지 알아보고자 한다. 이와 관련하여 일원 분산 분석을
+# 실시하고 p-value를 기술하시오.
+# - p-value는 반올림하여 소수점 둘째 자리까지 기술하시오. (답안 예시) 0.12
+# 
+# (참고)
+# from statsmodels.formula.api import ols
+# from statsmodels.stats.anova import anova_lm
+# =============================================================================
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
