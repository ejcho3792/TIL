{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국일보 뉴스 말뭉치\n",
    "# http://kristalinfo.dynu.net/TestCollections/\n",
    "# hkib-20000-40075.tar.gz \n",
    "# ./data/\n",
    "\n",
    "import os\n",
    "import re # 정규표현식 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Kkma, Hannanum\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_prefix='./data/'\n",
    "target_dir='HKIB-20000'\n",
    "cat_dirs=['health','economy','science','education','culture','society','industry','leisure','politics']\n",
    "cat_prefixes=['건강','경제','과학','교육','문화','사회','산업','여가','정치']\n",
    "files=os.listdir(dir_prefix+target_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(dir_prefix + target_dir)\n",
    "\n",
    "for file in files:\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    with open(dir_prefix + target_dir + '/' + file) as currfile:\n",
    "        doc_cnt = 0\n",
    "        docs = []\n",
    "        curr_doc = None\n",
    "        \n",
    "        for curr_line in currfile:\n",
    "            if curr_line.startswith('@DOCUMENT'):\n",
    "                if curr_doc is not None:\n",
    "                    docs.append(curr_doc)\n",
    "                curr_doc = curr_line\n",
    "                doc_cnt = doc_cnt + 1\n",
    "                continue\n",
    "            curr_doc = curr_doc + curr_line\n",
    "        \n",
    "        for doc in docs:\n",
    "            doc_lines = doc.split('\\n')\n",
    "            doc_no = doc_lines[1][9:]\n",
    "            \n",
    "            doc_cat03 = ''\n",
    "            for line in doc_lines[:10]:\n",
    "                if line.startswith(\"#CAT'03:\"):\n",
    "                    doc_cat03 = line[10:]\n",
    "                    break\n",
    "            \n",
    "            for cat_prefix in cat_prefixes:\n",
    "                if doc_cat03.startswith(cat_prefix):\n",
    "                    dir_index = cat_prefixes.index(cat_prefix)\n",
    "                    break\n",
    "                    \n",
    "            filtered_lines = []\n",
    "            for line in doc_lines:\n",
    "                if not (line.startswith('#') or line.startswith('@')):\n",
    "                    filtered_lines.append(line)\n",
    "                    \n",
    "            filename = 'hkib-' + doc_no + '.txt'\n",
    "            filepath = dir_prefix + target_dir + '/' + cat_dirs[dir_index]\n",
    "            \n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            f = open(filepath + '/' + filename, 'w')\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirs=['education','health']\n",
    "for i, d in enumerate(dirs):\n",
    "    files=os.listdir(dir_prefix+target_dir+'/'+d)\n",
    "    for file in files:\n",
    "        f=open(dir_prefix+target_dir+'/'+d+'/'+file,'r',encoding='utf-8')\n",
    "        raw=f.read()\n",
    "        reg_raw = re.sub(r'[-\\'@#:/◆▲0-9a-zA-Z<>!-\"*\\(\\)]', '', raw)\n",
    "        reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "        reg_raw = reg_raw.replace('\\n', ' ')\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_ls=[]\n",
    "y_ls=[]\n",
    "tmp1=[]\n",
    "tmp2=''\n",
    "tokenizer=Kkma()\n",
    "\n",
    "for i,d in enumerate(dirs):\n",
    "    files=os.listdir(dir_prefix+target_dir+'/'+d)\n",
    "    \n",
    "    for file in files:\n",
    "        f=open(dir_prefix+target_dir+'/'+d+'/'+file,'r',encoding='utf-8')\n",
    "        raw=f.read()\n",
    "        reg_raw = re.sub(r'[-\\'@#:/◆▲0-9a-zA-Z<>!-\"*\\(\\)]', '', raw)\n",
    "        reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "        reg_raw = reg_raw.replace('\\n', ' ')\n",
    "        tokens=tokenizer.nouns(reg_raw)\n",
    "        print(file)\n",
    "        for token in tokens:\n",
    "            tmp1.append(token)\n",
    "            \n",
    "        tmp2=' '.join(tmp1)\n",
    "        x_ls.append(tmp2)\n",
    "        tmp1=[]\n",
    "        \n",
    "        y_ls.append(i)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_ls)\n",
    "x_arr=np.array(x_ls)\n",
    "y_arr=np.array(y_ls)\n",
    "cntvec=CountVectorizer()\n",
    "x_cntvecs=cntvec.fit_transform(x_arr)\n",
    "x_cntarray=x_cntvecs.toarray()\n",
    "pd.DataFrame(x_cntarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(cntvec.vocabulary_.items(),key=lambda x:x[1]):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec=TfidfVectorizer(use_idf=True)\n",
    "x_tf_vec=tfidf_vec.fit_transform(x_arr)\n",
    "x_tf_arr=x_tf_vec.toarray()\n",
    "pd.DataFrame(x_tf_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx,vx,ty,vy=model_selection.train_test_split(x_tf_arr,y_arr,test_size=0.2)\n",
    "print(len(tx),len(vx))\n",
    "train=TensorDataset(tx,ty)\n",
    "print(train[0])\n",
    "\n",
    "train_loader=DataLoader(train, batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(33572,256)\n",
    "        self.fc1=nn.Linear(256,256)\n",
    "        self.fc1=nn.Linear(256,256)\n",
    "        self.fc1=nn.Linear(256,128)\n",
    "        self.fc1=nn.Linear(128,128)\n",
    "        self.fc1=nn.Linear(128,2)\n",
    "        \n",
    "    def forward(self,x):    \n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=F.relu(self.fc4(x))\n",
    "        x=F.relu(self.fc5(x))\n",
    "        x=self.fc6(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.005)\n",
    "for epoch in range(20):\n",
    "    total_loss=0\n",
    "    for tx,ty in train_loader:\n",
    "        tx,ty=Variable(tx),Variable(ty)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(tx)\n",
    "        loss=criterion(output,ty)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.data[0]\n",
    "        \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(epoch+1,total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx,vy=Variable(vx),Variable(vy)\n",
    "res=torch.max(model(vx).data, 1)[1]\n",
    "acc=sum(vy.numpy()==res.numpy()/len(vy.numpy()))\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
